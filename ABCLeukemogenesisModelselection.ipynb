{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea2f60c",
   "metadata": {},
   "source": [
    "# Model leukemia development and determine posterior distributions with pyABC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2b4d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "#from sympy import *\n",
    "import pandas as pds\n",
    "#import pymc3 as pm\n",
    "import pyabc as pyabc\n",
    "import os\n",
    "#from numba import jit\n",
    "import scipy.stats as st\n",
    "from scipy.special import kl_div\n",
    "import tempfile\n",
    "from sklearn.metrics import mean_log_error\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9497051-32a8-4b1f-b404-2d03a7027d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/omics/groups/OE0603/internal/jolly/Buettner_Rieger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f4b018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to sample from mixture distribution\n",
    "def sample_from_mixture_gamma(num_samples,alpha1,beta1,alpha2,beta2,mixing_prob):\n",
    "    '''sample from a mixture gamma distribution\n",
    "    Arguments:\n",
    "    num_samples  -- sample size\n",
    "    alpha1       -- shape parameter of the first gamma distr.\n",
    "    beta1        -- rate parameters of the first gamma distr.\n",
    "    alpha2       -- shape parameter of the sec. gamma distr.\n",
    "    beta2        -- rate parameters of the sec. gamma distr.\n",
    "    mixing_prob  -- probability to sample from the first gamma distr.\n",
    "    '''\n",
    "    # Generate random probabilities for each sample\n",
    "    random_probs = np.random.random(num_samples)\n",
    "    # Create a boolean mask for samples to be drawn from the first gamma distribution\n",
    "    mask = random_probs < mixing_prob\n",
    "    # Generate samples from the mixture gamma distribution\n",
    "    samples = np.empty(num_samples)\n",
    "    samples[mask] = np.random.gamma(alpha1, scale=beta1, size=mask.sum())\n",
    "    samples[~mask] = np.random.gamma(alpha2, scale=beta2, size=(~mask).sum())\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7310f1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_y(params):\n",
    "    '''analytical solution of the ODE model for one-way differentiation\n",
    "    Arguments:\n",
    "    p_1          -- proliferation rate of cluster of origin\n",
    "    p_2          -- proliferation rate of cluster 2\n",
    "    p_3          -- proliferation rate of cluster 3\n",
    "    d_1_2        -- differentiation rate from cluster of origin to cluster 2\n",
    "    d_1_3        -- differentiation rate from cluster of origin to cluster 3\n",
    "    t            -- time (in days)\n",
    "    n_1          -- initial number of cells in cluster of origin\n",
    "    '''\n",
    "    p_1, p_2,p_3,d_1_2,d_1_3,d_2_3,t, n_1 = params\n",
    "    y_1 = n_1*np.exp(-t*(d_1_2 + d_1_3 - p_1))\n",
    "    y_2 = d_1_2*n_1*np.exp(p_2*t)/(d_1_2 + d_1_3 - p_1 + p_2) - d_1_2*n_1*np.exp(-t*(d_1_2 + d_1_3 - p_1))/(d_1_2 + d_1_3 - p_1 + p_2)\n",
    "    y_3 = d_1_3*n_1*np.exp(p_3*t)/(d_1_2 + d_1_3 - p_1 + p_3) - d_1_3*n_1*np.exp(-t*(d_1_2 + d_1_3 - p_1))/(d_1_2 + d_1_3 - p_1 + p_3)\n",
    "    return y_1, y_2, y_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a04ef2-5933-44aa-b07f-e820d452bcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "\n",
    "# Define the P matrix (numerically substituted)\n",
    "\n",
    "def calculate_y_back(params):\n",
    "    '''ODE model for plasticity\n",
    "            Arguments:\n",
    "    p_1          -- proliferation rate of cluster of origin\n",
    "    p_2          -- proliferation rate of cluster 2\n",
    "    p_3          -- proliferation rate of cluster 3\n",
    "    d_1_2        -- differentiation rate between cluster of origin and cluster 2\n",
    "    d_1_3        -- differentiation rate between cluster of origin and cluster 3\n",
    "    d_2_3        -- differentiation rate between cluster 2 and cluster 3    \n",
    "    t            -- time (in days)\n",
    "    n_1          -- initial number of cells in cluster of origin\n",
    "    '''\n",
    "    p_1, p_2,p_3,d_1_2,d_1_3,d_2_3 ,t, n_1= params[0:8]    \n",
    "    def ode_system(t,y,p_1, p_2,p_3,d_1_2,d_1_3,d_2_3):\n",
    "        dydt = [\n",
    "        (p_1 -  d_1_2 + d_1_3) * y[0] + d_1_2 * y[1] + d_1_3 * y[2],  # dy1/dt\n",
    "        d_1_2 * y[0] + (p_2 - d_1_2-d_2_3) * y[1]+d_2_3 * y[2],                  # dy2/dt\n",
    "        d_1_3 * y[0] + (p_3 - d_1_3-d_2_3) * y[2]+d_2_3 * y[1]                # dy3/dt\n",
    "        ]\n",
    "        return dydt\n",
    "    y0 = [n_1, 0, 0]  \n",
    "    t_span = (0, t)  \n",
    "# Solve the system\n",
    "    solution = solve_ivp(ode_system, t_span, y0, t_eval=[t], method='RK45',args=(p_1, p_2,p_3,d_1_2,d_1_3,d_2_3))\n",
    "    sol = solution.y\n",
    "    result = tuple(np.float64(x) for x in sol.flatten())\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6618d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(paraMatrix):\n",
    "    ''' simulates the one-way differentiation model for every clones, returns population size at for each clone and each cluster at t end\n",
    "    '''\n",
    "    iterable=tuple(paraMatrix)\n",
    "    Simres=np.asanyarray(list(map(lambda params: calculate_y(params),iterable)))\n",
    "    return Simres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e28855b-8483-45d7-a4c7-f17f838ecf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_backAndForth(paraMatrix):\n",
    "    ''' simulates the plasticity model for every clones,, returns population size at for each clone and each cluster at t end\n",
    "    '''\n",
    "    iterable=tuple(paraMatrix)\n",
    "    Simres=np.asanyarray(list(map(lambda params: calculate_y_back(params),iterable)))\n",
    "    return Simres "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93776509",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_clones = 200 # number of clones \n",
    "num_param= 6\n",
    "\n",
    "\n",
    "T=50 # Final time of simulation, time of death in \n",
    "paraMatrix = np.zeros([num_clones,num_param+1])\n",
    "\n",
    "\n",
    "parameters={\"M_d_1_2_rate\":0.166524,\"alpha_d\":553.402914,\"mixing_prob_1_2\":1.351263,\"M_d_1_3_rate\":0.689444,\"mixing_prob_1_3\":0.579466,\"alpha_pr\":84.913724,\"M_pr1_rate\":0.169292,\"M_pr2_rate\":0.37532,\"M_pr3_rate\":0.36426,\"M_dlow_rate\":0.000001,\"mixing_prob_2_3\":0.579466,\"M_d_2_3_rate\":0.579466}\n",
    "def parameterMatrix(parameters,T):\n",
    "    '''Generates model parameters for each clones sampled from the distributions with parameters fitted by the ABC framework\n",
    "    '''\n",
    "    paraMatrix = np.zeros([num_clones,num_param+2]) # matrix of dynamical parameters for each clone\n",
    "    M_d_1_2_rate = parameters[\"M_d_1_2_rate\"] # average \"active\" differentiation rate cluster 1  <-> or -> cluster 2\n",
    "    alpha_d = parameters[\"alpha_d\"] # shape parameter of the distributions of differentiation rates \n",
    "    beta_d_1_2 = M_d_1_2_rate/alpha_d  # scale parameter of the distribution of active differentiation rates cluster 1 <-> or -> cluster 2\n",
    "    mixing_prob_1_2 = parameters[\"mixing_prob_1_2\"] # Probability of sampling from the first gamma distribution for differentiation rates  1 <-> or -> 2\n",
    "    M_d_1_3_rate= parameters[\"M_d_1_3_rate\"]# average \"active\" differentiation rate cluster 1 <-> or ->  cluster 3\n",
    "    beta_d_1_3 = M_d_1_3_rate/alpha_d   # scale parameter of the distribution for active differentiation rates cluster 1 <-> or -> cluster 3\n",
    "    M_dlow_rate = parameters[\"M_dlow_rate\"] # average low/background differentiation rate \n",
    "    beta_dlow = M_dlow_rate/alpha_d # scale parameter of the distribution for background differentiation rates\n",
    "    mixing_prob_1_3 = parameters[\"mixing_prob_1_3\"]  # Probability of sampling from the first gamma distribution for differentiation rates 1<-> or -> 3\n",
    "    #shape parameter is common to every compartment proliferation\n",
    "    alpha_pr = parameters[\"alpha_pr\"] # shape parameter of the distributions of proliferation rates \n",
    "    M_pr1_rate = parameters[\"M_pr1_rate\"] # average proliferation rate in cluster 1 \n",
    "    beta_pr1 = M_pr1_rate/alpha_pr # scale parameter of the distribution for proliferation rates in cluster 1 \n",
    "    M_pr2_rate = parameters[\"M_pr2_rate\"] # scaling factor for the proliferation rate in cluster 2\n",
    "    M_pr3_rate = parameters[\"M_pr3_rate\"] # scaling factor for the proliferation rate in cluster 3\n",
    "    p_r_1 = paraMatrix[:,0]\n",
    "    p_r_2 = paraMatrix[:,1]\n",
    "    p_r_3 = paraMatrix[:,2]\n",
    "    d_r_1_2= paraMatrix[:,3]\n",
    "    d_r_1_3 = paraMatrix[:,4]\n",
    "    p_r_1[0:num_clones] = np.random.gamma(alpha_pr, beta_pr1, size=num_clones) #sampled proliferation rates for each clone in cluster 1\n",
    "    p_r_2[0:num_clones] = p_r_1[0:num_clones]*M_pr2_rate # adjusted proliferation rates for cluster 2\n",
    "    p_r_3[0:num_clones] = p_r_1[0:num_clones]*M_pr3_rate  # adjusted proliferation rates for cluster 3\n",
    "    d_r_1_2[0:num_clones] = sample_from_mixture_gamma(num_clones,alpha_d,beta_dlow,alpha_d,beta_d_1_2,mixing_prob_1_2) #sampled differentiation rates for each clone cl 1 <-> or -> 2\n",
    "    d_r_1_3[0:num_clones] = sample_from_mixture_gamma(num_clones,alpha_d,beta_dlow,alpha_d,beta_d_1_3,mixing_prob_1_3) #sampled differentiation rates for each clone cl 1 <-> or ->  3\n",
    "    d_r_2_3 = paraMatrix[:,5]\n",
    "    M_d_2_3_rate= parameters[\"M_d_2_3_rate\"] # average \"active\" differentiation rate cluster 2  <->  cluster 3\n",
    "    mixing_prob_2_3 = parameters[\"mixing_prob_2_3\"] #probability of sampling from the first gamma distribution for differentiation rates  2 <-> 3\n",
    "    beta_d_2_3 = M_d_2_3_rate/alpha_d # scale parameter of the distribution of active differentiation rate cluster 2  <->  cluster 3\n",
    "    d_r_2_3 = sample_from_mixture_gamma(num_clones,alpha_d,beta_dlow,alpha_d,beta_d_2_3,mixing_prob_2_3) #sampled differentiation rates for each clone cl 2 <-> 3\n",
    "    paraMatrix[:,6] = np.repeat(T,num_clones) # final time \n",
    "    #and now for the initial number of cells per clone in cluster 1 \n",
    "    mean_n = 5\n",
    "    alpha_n = 10\n",
    "    beta_n = mean_n / alpha_n \n",
    "    paraMatrix[:,7] = np.round(np.random.gamma(alpha_n, beta_n, size=num_clones))\n",
    "    return paraMatrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9b9666-e02a-4fac-99f5-92b7b1120335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ordered_outPut(Simres,DataSize,sample,OutputOrder):\n",
    "    '''from the result of the model simulation, generate samples of the same size as the overall single-cell dataset, then \n",
    "    Argument:\n",
    "    Simres       -- output of model() or modelbackandforth() functions\n",
    "    DataSize     -- total number of single cell for the given mouse\n",
    "    sample       -- proportion of non barcoded cells in the data per cluster \n",
    "    OutputOrder  -- 3 possible ordering of clusters with the source cluster positionned in 1st, 2nd or 3rd column\n",
    "    '''\n",
    "    flat_replications = Simres.flatten(order='F')\n",
    "    flat_replications=flat_replications.astype(\"float64\")\n",
    "    flat_indices = np.arange(0,len(flat_replications),1)\n",
    "    sampled_items = np.random.choice(flat_indices, size=DataSize, p=flat_replications/np.sum(flat_replications)) # sampling as many cells as there are in the datasets\n",
    "    item_counts = np.bincount(sampled_items,minlength=num_clones*3)\n",
    "    item_counts_clust1 = item_counts[0:num_clones]\n",
    "    item_counts_clust2 = item_counts[num_clones:num_clones*2]\n",
    "    item_counts_clust3 = item_counts[num_clones*2:num_clones*3]\n",
    "    total_counts = item_counts_clust1+item_counts_clust2+item_counts_clust3\n",
    "    def reduce_cluster(cluster, sample): #sampling the same proporition of labeled cells per cluster as in the data\n",
    "        sample_size=np.sum(cluster)-np.int64(sample*np.sum(cluster))\n",
    "        expanded_cluster = np.repeat(np.arange(len(cluster)), cluster)\n",
    "        sampled_counts = np.random.choice(expanded_cluster, size=sample_size, replace=False)\n",
    "        return np.bincount(sampled_counts, minlength=len(cluster))\n",
    "    # ordering the output with source cluster in 1st, 2nd or 3rd column\n",
    "    if OutputOrder==0:\n",
    "        ordered_clust2red = reduce_cluster(item_counts_clust2, sample[0])\n",
    "        ordered_clust1red = reduce_cluster(item_counts_clust1, sample[1])\n",
    "        ordered_clust3red = reduce_cluster(item_counts_clust3, sample[2])\n",
    "        total_counts= ordered_clust2red+ordered_clust1red+ordered_clust3red\n",
    "        order=np.argsort(total_counts)[-len(total_counts):][::-1]\n",
    "        ToReturn = np.vstack([ordered_clust2red[order], ordered_clust1red[order], ordered_clust3red[order]]).T        \n",
    "    # Cluster 1, 2, 3\n",
    "    elif OutputOrder==1:\n",
    "        ordered_clust1red = reduce_cluster(item_counts_clust1, sample[0])\n",
    "        ordered_clust2red = reduce_cluster(item_counts_clust2, sample[1])\n",
    "        ordered_clust3red = reduce_cluster(item_counts_clust3, sample[2])\n",
    "        total_counts= ordered_clust2red+ordered_clust1red+ordered_clust3red\n",
    "        order=np.argsort(total_counts)[-len(total_counts):][::-1]\n",
    "        ToReturn = np.vstack([ordered_clust1red[order], ordered_clust2red[order], ordered_clust3red[order]]).T\n",
    "     # Cluster 2, 3, 1\n",
    "    elif OutputOrder==2:\n",
    "        ordered_clust2red = reduce_cluster(item_counts_clust2, sample[0])\n",
    "        ordered_clust3red = reduce_cluster(item_counts_clust3, sample[1])\n",
    "        ordered_clust1red = reduce_cluster(item_counts_clust1, sample[2])\n",
    "        total_counts = ordered_clust2red+ordered_clust1red+ordered_clust3red\n",
    "        order = np.argsort(total_counts)[-len(total_counts):][::-1]\n",
    "        ToReturn = np.vstack([ordered_clust2red[order], ordered_clust3red[order], ordered_clust1red[order]]).T\n",
    "    overall_sum = np.sum(ToReturn)\n",
    "    # compute the relative size of each clone\n",
    "    row_fractions = np.sum(ToReturn, axis=1) / overall_sum\n",
    "    # For each clone, compute the distribution across clusters\n",
    "    row_sums = np.sum(ToReturn, axis=1, keepdims=True)  # Row sums\n",
    "    row_distributions = np.zeros_like(ToReturn, dtype=float)  # Initialize distribution matrix\n",
    "    # Avoid division by zero for rows with sum = 0\n",
    "    non_zero_rows = row_sums.squeeze() > 0 \n",
    "    row_distributions[non_zero_rows] = ToReturn[non_zero_rows] / row_sums[non_zero_rows]   \n",
    "    ToReturn = np.hstack((row_distributions,row_fractions[:, np.newaxis]))\n",
    "    ToReturn = ToReturn.flatten(order=\"F\")\n",
    "    ToReturn = ToReturn.astype(float)\n",
    "    return ToReturn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad71f2b-6cb3-45cf-9233-9e5b4db35c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_outputs(returnedOutput1,returnedOutput2):\n",
    "    '''concatenate outputs of get_ordered_outPut() for the 2 mice\n",
    "    '''\n",
    "    final_output = np.concatenate((returnedOutput1,returnedOutput2))\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9afa0b7-64d5-4d53-b0db-8dce65472f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data into proper format\n",
    "\n",
    "data_1=pds.read_csv(\"latestbarcodeDist72.csv\")\n",
    "data_2=pds.read_csv(\"latestbarcodeDist73.csv\")\n",
    "num_clones = 200\n",
    "\n",
    "dataNP1=data_1.to_numpy()\n",
    "sample_1=dataNP1[0,1:4]/np.sum(dataNP1[:,1:4],axis=0)\n",
    "DataSize_1=np.sum(dataNP1[:,1:4])\n",
    "dataNP1=dataNP1[1:,:]\n",
    "orderdataNP1=np.argsort(np.sum(dataNP1[:,1:4],axis=1))[::-1]\n",
    "np.arange(0,len(orderdataNP1),1)\n",
    "backToOrder=[np.arange(0,len(orderdataNP1),1),orderdataNP1]\n",
    "ordereddataNP1=dataNP1[orderdataNP1,:]\n",
    "\n",
    "import tempfile\n",
    "\n",
    "data1clean = ordereddataNP1[:,1:4]\n",
    "\n",
    "data1clean=np.asarray(data1clean)\n",
    "\n",
    "#we add as many clones as simulated+1, the unlabeled\n",
    "diff=(num_clones)-np.shape(data1clean)[0]\n",
    "\n",
    "data1clean=np.vstack((data1clean, np.zeros([diff, 3])))\n",
    "\n",
    "\n",
    "### getting porportion of cells (relative size of each clone and distribution across clusters per clone)\n",
    "\n",
    "overall_sum = np.sum(data1clean)\n",
    "row_fractions = np.sum(data1clean, axis=1) / overall_sum\n",
    "\n",
    "# Compute row sums and identify non-zero rows\n",
    "row_sums = np.sum(data1clean, axis=1, keepdims=True) \n",
    "non_zero_rows = row_sums.squeeze() > 0  # Boolean mask for rows with non-zero sum\n",
    "\n",
    "# Initialize row_distributions with zeros\n",
    "row_distributions = np.zeros_like(data1clean, dtype=float)\n",
    "\n",
    "# Only normalize rows with non-zero sums\n",
    "row_distributions[non_zero_rows] = data1clean[non_zero_rows] / row_sums[non_zero_rows]\n",
    "\n",
    "data1clean=np.hstack((row_distributions,row_fractions[:, np.newaxis]))\n",
    "data1clean=data1clean.flatten(order='F')\n",
    "data1clean=data1clean.astype(float)\n",
    "\n",
    "################################\n",
    "########same as above for mouse 2\n",
    "################################\n",
    "\n",
    "dataNP2=data_2.to_numpy()\n",
    "sample_2=dataNP2[0,1:4]/np.sum(dataNP2[:,1:4],axis=0)\n",
    "DataSize_2=np.sum(dataNP2[:,1:4])\n",
    "dataNP2=dataNP2[1:,:]\n",
    "orderdataNP2=np.argsort(np.sum(dataNP2[:,1:4],axis=1))[::-1]\n",
    "np.arange(0,len(orderdataNP2),1)\n",
    "backToOrder=[np.arange(0,len(orderdataNP2),1),orderdataNP2]\n",
    "ordereddataNP2=dataNP2[orderdataNP2,:]\n",
    "#ordereddataNP2[:,1:4]=ordereddataNP2[:,1:4]/np.sum(ordereddataNP2[:,1:4],axis=0)\n",
    "\n",
    "import tempfile\n",
    "\n",
    "data2clean = ordereddataNP2[:,1:4]\n",
    "#abc.new(\"sqlite:///mouse73CloneSizeDist.db\" , {\"data\": data2clean})\n",
    "data2clean=np.asarray(data2clean)\n",
    "#we add as many clones as simulated+1, the unlabeled\n",
    "diff=(num_clones)-np.shape(data2clean)[0]\n",
    "#print(data2clean)\n",
    "#print(np.zeros([42,4]))\n",
    "#print(ordereddataNP2[:,1:4])\n",
    "data2clean=np.vstack((data2clean, np.zeros([diff, 3])))\n",
    "#print(data2clean)\n",
    "#print(DataSize)\n",
    "#print(ordereddataNP2)\n",
    "overall_sum = np.sum(data2clean)\n",
    "row_fractions = np.sum(data2clean, axis=1) / overall_sum\n",
    "\n",
    "# Compute row sums and identify non-zero rows\n",
    "row_sums = np.sum(data2clean, axis=1, keepdims=True)  # Keep dimensions for broadcasting\n",
    "non_zero_rows = row_sums.squeeze() > 0  # Boolean mask for rows with non-zero sum\n",
    "\n",
    "# Initialize row_distributions with zeros\n",
    "row_distributions = np.zeros_like(data2clean, dtype=float)\n",
    "\n",
    "# Only normalize rows with non-zero sums\n",
    "row_distributions[non_zero_rows] = data2clean[non_zero_rows] / row_sums[non_zero_rows]\n",
    "\n",
    "data2clean=np.hstack((row_distributions,row_fractions[:, np.newaxis]))\n",
    "\n",
    "\n",
    "data2clean=data2clean.flatten(order='F')\n",
    "data2clean=data2clean.astype(float)\n",
    "\n",
    "\n",
    "# concatenate the 2 mice\n",
    "sample=list([sample_1,sample_2])\n",
    "\n",
    "\n",
    "observation=np.concatenate((data1clean,data2clean))\n",
    "\n",
    "DataSize=list([DataSize_1,DataSize_2])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870ddaef-42dc-4c17-8134-adc74f56fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bigModels:\n",
    "    '''generates wrapper functions which produce final outputs to be compared to data from a parameter set\n",
    "    Argument:\n",
    "    DataSize     -- list with 2 elements: total number of single cell for each of the mice\n",
    "    T            -- T end for the simulations\n",
    "    sample       -- proportion of non barcoded cells in the data per cluster (column) and per mouse (row)\n",
    "    OutputOrder  -- 3 possible ordering of clusters with the source cluster positionned in 1st, 2nd or 3rd column\n",
    "    '''    \n",
    "    def __init__(self,DataSize,T,sample,OutputOrder):\n",
    "          self.DataSize, self.T,self.sample,self.OutputOrder = DataSize,T,sample,OutputOrder\n",
    "    def __call__(self, parameters):\n",
    "        paraMatrix_1=parameterMatrix(parameters,self.T)\n",
    "        paraMatrix_2=parameterMatrix(parameters,self.T)\n",
    "        Simres_1=model(paraMatrix_1)\n",
    "        Simres_2=model(paraMatrix_2)\n",
    "        #print(Simres)\n",
    "        ordered_outPut1 = get_ordered_outPut(Simres_1,self.DataSize[0],self.sample[0],self.OutputOrder)\n",
    "        ordered_outPut2 = get_ordered_outPut(Simres_2,self.DataSize[1],self.sample[1],self.OutputOrder)\n",
    "        ordered_output = concatenate_outputs(ordered_outPut1,ordered_outPut2)\n",
    "        return {\"data\": ordered_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a3106f-7867-482d-8a11-6d12406584fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bigModels_back:\n",
    "     '''same as bigModels for plasticity models\n",
    "    '''    \n",
    "    def __init__(self,DataSize,T,sample,OutputOrder):\n",
    "          self.DataSize, self.T,self.sample,self.OutputOrder = DataSize,T,sample,OutputOrder\n",
    "    def __call__(self, parameters):\n",
    "        paraMatrix_1=parameterMatrix(parameters,self.T)\n",
    "        paraMatrix_2=parameterMatrix(parameters,self.T)\n",
    "        Simres_1=model_backAndForth(paraMatrix_1)\n",
    "        Simres_2=model_backAndForth(paraMatrix_2)\n",
    "        #print(Simres)\n",
    "        ordered_outPut1 = get_ordered_outPut(Simres_1,self.DataSize[0],self.sample[0],self.OutputOrder)\n",
    "        ordered_outPut2 = get_ordered_outPut(Simres_2,self.DataSize[1],self.sample[1],self.OutputOrder)\n",
    "        ordered_output =concatenate_outputs(ordered_outPut1,ordered_outPut2)\n",
    "        return {\"data\": ordered_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ac3c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mean_variance_per_block(data, block_size=6, num_blocks=7):\n",
    "    \"\"\"\n",
    "    Compute mean and variance per column for blocks of rows.\n",
    "\n",
    "    Parameters:\n",
    "    - data: np.array of shape (>=30, 3) where we compute stats on the first 30 rows.\n",
    "    - block_size: Size of each block (default=10).\n",
    "    - num_blocks: Number of blocks to compute (default=3).\n",
    "\n",
    "    Returns:\n",
    "    - means: np.array of shape (num_blocks, 3) with mean per column per block.\n",
    "    - variances: np.array of shape (num_blocks, 3) with variance per column per block.\n",
    "    \"\"\"\n",
    "    means = []\n",
    "    variances = []\n",
    "\n",
    "    for i in range(num_blocks):\n",
    "        block = data[i * block_size : (i + 1) * block_size]  # Extract block\n",
    "        means.append(np.mean(block, axis=0))  # Mean per column\n",
    "        variances.append(np.std(block, axis=0))  # Variance per column\n",
    "\n",
    "    return np.array(means), np.array(variances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a0eb51-c87b-4ca2-9109-3db40cf45e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(sim_data,obs_data, block_size=6, num_blocks=7):\n",
    "    \"\"\"\n",
    "    Compute distance between observed and simulated block means and variances.\n",
    "    Parameters:\n",
    "    - obs_data: np.array of shape (>=30, 3) - first 30 rows of observed data.\n",
    "    - sim_data: np.array of shape (>=30, 3) - first 30 rows of simulated data.\n",
    "    - block_size: Number of rows per block (default=10).\n",
    "    - num_blocks: Number of blocks (default=3).\n",
    "    Returns:\n",
    "    - total_distances: np.array of shape (num_blocks,), total  distance per block.\n",
    "    \"\"\" \n",
    "    obs_data_1 = obs_data[\"data\"][0:num_clones*4]\n",
    "    sim_data_1 = sim_data[\"data\"][0:num_clones*4]\n",
    "    obs_data_2 = obs_data[\"data\"][num_clones*4:]\n",
    "    sim_data_2 = sim_data[\"data\"][num_clones*4:]\n",
    "    shapes=np.shape(obs_data_1)\n",
    "\n",
    "    # compute for first mouse\n",
    "    obs_data_1 = obs_data_1.reshape((np.int64(shapes[0]/4),4),order=\"F\")\n",
    "    obs_data_1 = obs_data_1[:, [0, 1,2, 3]]\n",
    "    sim_data_1 = np.reshape(sim_data_1,shape=(np.int64(shapes[0]/4),4),order=\"F\")\n",
    "    sim_data_1 = sim_data_1[:, [0, 1,2, 3]]\n",
    "    #obs_data=obs_data[:,0:4]\n",
    "    #sim_data=sim_data[:,0:4]\n",
    "    # Compute block-wise means and variances for observed and simulated data\n",
    "    obs_means_1, obs_variances_1 = mean_variance_per_block(obs_data_1, block_size, num_blocks)\n",
    "    sim_means_1, sim_variances_1 = mean_variance_per_block(sim_data_1, block_size, num_blocks)\n",
    "\n",
    "    # Compute  Euclidean distance for means\n",
    "    distance_means_1 = np.sum(np.abs(obs_means_1 - sim_means_1), axis=1)\n",
    "\n",
    "    # Compute  Euclidean distance for variances\n",
    "    distance_variances_1 = np.sum(np.abs(obs_variances_1 - sim_variances_1), axis=1)\n",
    "\n",
    "    # Total  distance = sum of both\n",
    "    total_distances_1 = distance_means_1 + distance_variances_1\n",
    "\n",
    "    # compute for second mouse\n",
    "    \n",
    "    obs_data_2 = obs_data_2.reshape((np.int64(shapes[0]/4),4),order=\"F\")\n",
    "    obs_data_2 = obs_data_2[:, [0, 1,2, 3]]\n",
    "    sim_data_2 = np.reshape(sim_data_2,shape=(np.int64(shapes[0]/4),4),order=\"F\")\n",
    "    sim_data_2 = sim_data_2[:, [0, 1,2, 3]]\n",
    "    #obs_data=obs_data[:,0:4]\n",
    "    #sim_data=sim_data[:,0:4]\n",
    "    # Compute block-wise means and variances for observed and simulated data\n",
    "    obs_means_2, obs_variances_2 = mean_variance_per_block(obs_data_2, block_size, num_blocks)\n",
    "    sim_means_2, sim_variances_2 = mean_variance_per_block(sim_data_2, block_size, num_blocks)\n",
    "\n",
    "    # Compute  Euclidean distance for means\n",
    "    distance_means_2 = np.sum(np.abs(obs_means_2 - sim_means_2), axis=1)\n",
    "    # Compute  Euclidean distance for variances\n",
    "    distance_variances_2 = np.sum(np.abs(obs_variances_2 - sim_variances_2), axis=1)\n",
    "\n",
    "    # Total  distance = sum of both\n",
    "    total_distances = distance_means_1 + distance_variances_1 + distance_means_2 + distance_variances_2\n",
    "    return np.sum(total_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b152b017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to simulate each of the 6 models\n",
    "bigModel_1=bigModels(DataSize,T,sample,0)\n",
    "bigModel_2=bigModels(DataSize,T,sample,1)\n",
    "bigModel_3=bigModels(DataSize,T,sample,2)\n",
    "bigModel_4=bigModels_back(DataSize,T,sample,0)\n",
    "bigModel_5=bigModels_back(DataSize,T,sample,1)\n",
    "bigModel_6=bigModels_back(DataSize,T,sample,2)\n",
    "\n",
    "\n",
    "models = [bigModel_1,bigModel_2,bigModel_3,bigModel_4,bigModel_5,bigModel_6]\n",
    "\n",
    "# prior distribution, shared by all 6 models\n",
    "prior=pyabc.Distribution(M_d_1_2_rate=pyabc.RV(\"uniform\", 0.1, 0.6),alpha_d=pyabc.RV(\"uniform\", 200, 600),mixing_prob_1_2=pyabc.RV(\"uniform\", 0.5, 0.4),\n",
    "M_d_1_3_rate=pyabc.RV(\"uniform\", 0.1, 0.6),mixing_prob_1_3=pyabc.RV(\"uniform\", 0.5, 0.4),\n",
    "alpha_pr=pyabc.RV(\"uniform\", 5, 1000),M_pr1_rate=pyabc.RV(\"uniform\", 0.1, 0.6),M_pr2_rate=pyabc.RV(\"norm\", 1, 0.3),M_pr3_rate=pyabc.RV(\"norm\", 1, 0.3),M_dlow_rate=pyabc.RV(\"uniform\", 0.000001, 0.000001),mixing_prob_2_3=pyabc.RV(\"uniform\", 0.5, 0.5),M_d_2_3_rate=pyabc.RV(\"uniform\", 0.1, 0.6))\n",
    "\n",
    "\n",
    "priors = [prior,prior,prior,prior,prior,prior]\n",
    "\n",
    "\n",
    "# prepare sampler\n",
    "abc = pyabc.ABCSMC(models, priors, distance, population_size=8000,sampler=pyabc.MulticoreEvalParallelSampler(n_procs=8))#,sampler =pyabc.SingleCoreSampler())\n",
    "\n",
    "\n",
    "db_path = os.path.join(tempfile.gettempdir(), \"sim.db\")\n",
    "history = abc.new(\"sqlite:///\" + db_path, {\"data\": observation})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b198a667-88e0-487e-898c-cda117df9189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run sampler\n",
    "abc.run(min_acceptance_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4180e906",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_probabilities = history.get_model_probabilities()\n",
    "model_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbcaa70-f2ce-403c-b894-162930bbf4fe",
   "metadata": {},
   "source": [
    "# Generate new model simulations from the posterior distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d711168f-63d5-4e95-a4ba-80a0cb1f6705",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#prepare posterior distribution for model 4 to use as input for model simulation\n",
    "\n",
    "model_3_data = df_pop[df_pop['m'] == 3]\n",
    "\n",
    "# Now, get the 'sumstat_val' for those rows and group every 10 rows into an array\n",
    "sumstat_values_model_3 = []\n",
    "\n",
    "# Iterate over the rows in chunks of 12\n",
    "for i in range(0, len(model_3_data), 12):\n",
    "    # Extract the sumstat_val for the current set of 12 rows\n",
    "    sumstat_chunk = model_3_data['sumstat_val'].to_numpy()[i]\n",
    "    sumstat_values_model_3.append(sumstat_chunk)\n",
    "\n",
    "par_names_model_3 = []\n",
    "\n",
    "for i in range(0, len(model_3_data), 12):\n",
    "    # Extract the sumstat_val for the current set of 10 rows\n",
    "    parnames_chunk = model_3_data['par_name'].to_numpy()[i:i+12]\n",
    "    par_names_model_3.append(parnames_chunk)\n",
    "\n",
    "par_values_model_3 = []\n",
    "\n",
    "for i in range(0, len(model_3_data), 12):\n",
    "    # Extract the sumstat_val for the current set of 12 rows\n",
    "    par_chunk = model_3_data['par_val'].to_numpy()[i:i+12]\n",
    "    par_values_model_3.append(par_chunk)\n",
    "\n",
    "dictPred_3=[]\n",
    "\n",
    "for i in range(0, len(par_names_model_3)):\n",
    "    dictPred_3.append(dict(zip(par_names_model_3[i],par_values_model_3[i])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e895f2c-b073-4cee-9c15-bf609f365636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we prepare posterior distribution for model 1\n",
    "\n",
    "model_0_data = df_pop[df_pop['m'] == 0]\n",
    "\n",
    "# Now, get the 'sumstat_val' for those rows and group every 10 rows into an array\n",
    "sumstat_values_model_0 = []\n",
    "\n",
    "# Iterate over the rows in chunks of 12\n",
    "for i in range(0, len(model_0_data), 12):\n",
    "    # Extract the sumstat_val for the current set of 12 rows\n",
    "    sumstat_chunk = model_0_data['sumstat_val'].to_numpy()[i]\n",
    "    sumstat_values_model_0.append(sumstat_chunk)\n",
    "\n",
    "par_names_model_0 = []\n",
    "\n",
    "for i in range(0, len(model_0_data), 12):\n",
    "    # Extract the sumstat_val for the current set of 10 rows\n",
    "    parnames_chunk = model_0_data['par_name'].to_numpy()[i:i+12]\n",
    "    par_names_model_0.append(parnames_chunk)\n",
    "\n",
    "par_values_model_0 = []\n",
    "\n",
    "for i in range(0, len(model_0_data), 12):\n",
    "    # Extract the sumstat_val for the current set of 12 rows\n",
    "    par_chunk = model_0_data['par_val'].to_numpy()[i:i+12]\n",
    "    par_values_model_0.append(par_chunk)\n",
    "\n",
    "dictPred_0=[]\n",
    "\n",
    "for i in range(0, len(par_names_model_0)):\n",
    "    dictPred_0.append(dict(zip(par_names_model_0[i],par_values_model_0[i])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707608cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_pop=history.get_all_populations()\n",
    "All_pop.to_csv(\"ModelfitPopCombinedCellPlasticity_200Clones_samp_PopStat.csv\")\n",
    "model_probabilities = history.get_model_probabilities()\n",
    "model_probabilities.to_csv(\"ModelfitPopCombinedCellPlasticity_200Clones_samp_proba.csv\")\n",
    "model_probabilitiesnump=model_probabilities.to_numpy()\n",
    "shape=np.shape(model_probabilitiesnump)[0]\n",
    "print(shape)\n",
    "df_pop=history.get_population_extended(t=shape-2)\n",
    "\n",
    "df_pop.to_csv(\"ModelfitPopCombinedCellPlasticity_200Clonessamp_posterior.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128f7baf-7297-4e7c-b241-abad893ad5d7",
   "metadata": {},
   "source": [
    "## return distances for each value of the summary statistics separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf294bb-7fc9-4cf0-94c1-76f2d3f15434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as distance() except distance_2 does not sum the distances\n",
    "\n",
    "def distance_2(sim_data,obs_data, block_size=6, num_blocks=7):\n",
    "    \"\"\"\n",
    "    Compute  distance between observed and simulated block means and variances.\n",
    "    Parameters:\n",
    "    - obs_data: np.array of shape (>=30, 3) - first 30 rows of observed data.\n",
    "    - sim_data: np.array of shape (>=30, 3) - first 30 rows of simulated data.\n",
    "    - block_size: Number of rows per block (default=10).\n",
    "    - num_blocks: Number of blocks (default=3).\n",
    "    Returns:\n",
    "    - total_distances: np.array of shape (num_blocks,), total  distance per block.\n",
    "    \"\"\" \n",
    "    obs_data_1 = obs_data[\"data\"][0:num_clones*4]\n",
    "    sim_data_1 = sim_data[\"data\"][0:num_clones*4]\n",
    "    obs_data_2 = obs_data[\"data\"][num_clones*4:]\n",
    "    sim_data_2 = sim_data[\"data\"][num_clones*4:]\n",
    "    shapes=np.shape(obs_data_1)\n",
    "\n",
    "    # compute for first mouse\n",
    "    obs_data_1 = obs_data_1.reshape((np.int64(shapes[0]/4),4),order=\"F\")\n",
    "    obs_data_1 = obs_data_1\n",
    "    sim_data_1 = np.reshape(sim_data_1,shape=(np.int64(shapes[0]/4),4),order=\"F\")\n",
    "    sim_data_1 = sim_data_1\n",
    "    #obs_data=obs_data[:,0:4]\n",
    "    #sim_data=sim_data[:,0:4]\n",
    "    # Compute block-wise means and variances for observed and simulated data\n",
    "    obs_means_1, obs_variances_1 = mean_variance_per_block(obs_data_1, block_size, num_blocks)\n",
    "    sim_means_1, sim_variances_1 = mean_variance_per_block(sim_data_1, block_size, num_blocks)\n",
    "\n",
    "    # Compute  Euclidean distance for means\n",
    "    distance_means_1 = np.abs(obs_means_1 - sim_means_1)\n",
    "\n",
    "    # Compute  Euclidean distance for variances\n",
    "    distance_variances_1 =np.abs(obs_variances_1 - sim_variances_1)\n",
    "\n",
    "    # Total distance = sum of both\n",
    "    total_distances_1 = distance_means_1 + distance_variances_1\n",
    "\n",
    "    # compute for second mouse\n",
    "    \n",
    "    obs_data_2 = obs_data_2.reshape((np.int64(shapes[0]/4),4),order=\"F\")\n",
    "    obs_data_2 = obs_data_2\n",
    "    sim_data_2 = np.reshape(sim_data_2,shape=(np.int64(shapes[0]/4),4),order=\"F\")\n",
    "    sim_data_2 = sim_data_2\n",
    "    #obs_data=obs_data[:,0:4]\n",
    "    #sim_data=sim_data[:,0:4]\n",
    "    # Compute block-wise means and variances for observed and simulated data\n",
    "    obs_means_2, obs_variances_2 = mean_variance_per_block(obs_data_2, block_size, num_blocks)\n",
    "    sim_means_2, sim_variances_2 = mean_variance_per_block(sim_data_2, block_size, num_blocks)\n",
    "\n",
    "    # Compute Euclidean distance for means\n",
    "     distance_means_2 = np.abs(obs_means_2 - sim_means_2)\n",
    "    # Compute Euclidean distance for variances\n",
    "    distance_variances_2 = np.abs(obs_variances_2 - sim_variances_2)\n",
    "\n",
    "    distances = np.vstack((distance_means_1,distance_variances_1,distance_means_2,distance_variances_2))\n",
    "   \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c61b802-ffff-47f2-a8c8-20852cb08d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distance_3 same as distance_2 but with relative distance instead of absolute\n",
    "\n",
    "def distance_3(sim_data,obs_data, block_size=6, num_blocks=7):\n",
    "    \"\"\"\n",
    "    Compute distance between observed and simulated block means and variances.\n",
    "    Parameters:\n",
    "    - obs_data: np.array of shape (>=30, 3) - first 30 rows of observed data.\n",
    "    - sim_data: np.array of shape (>=30, 3) - first 30 rows of simulated data.\n",
    "    - block_size: Number of rows per block (default=10).\n",
    "    - num_blocks: Number of blocks (default=3).\n",
    "    Returns:\n",
    "    - total_distances: np.array of shape (num_blocks,), total distance per block.\n",
    "    \"\"\" \n",
    "    obs_data_1 = obs_data[\"data\"][0:num_clones*4]\n",
    "    sim_data_1 = sim_data[\"data\"][0:num_clones*4]\n",
    "    obs_data_2 = obs_data[\"data\"][num_clones*4:]\n",
    "    sim_data_2 = sim_data[\"data\"][num_clones*4:]\n",
    "    shapes=np.shape(obs_data_1)\n",
    "\n",
    "    # compute for first mouse\n",
    "    obs_data_1 = obs_data_1.reshape((np.int64(shapes[0]/4),4),order=\"F\")\n",
    "    obs_data_1 = obs_data_1\n",
    "    sim_data_1 = np.reshape(sim_data_1,shape=(np.int64(shapes[0]/4),4),order=\"F\")\n",
    "    sim_data_1 = sim_data_1\n",
    "    #obs_data=obs_data[:,0:4]\n",
    "    #sim_data=sim_data[:,0:4]\n",
    "    # Compute block-wise means and variances for observed and simulated data\n",
    "    obs_means_1, obs_variances_1 = mean_variance_per_block(obs_data_1, block_size, num_blocks)\n",
    "    sim_means_1, sim_variances_1 = mean_variance_per_block(sim_data_1, block_size, num_blocks)\n",
    "\n",
    "    # Compute Euclidean distance for means\n",
    "    distance_means_1 = obs_means_1 - sim_means_1\n",
    "\n",
    "    # Compute Euclidean distance for variances\n",
    "    distance_variances_1 =obs_variances_1 - sim_variances_1\n",
    "\n",
    "    # Total distance = sum of both\n",
    "    total_distances_1 = distance_means_1 + distance_variances_1\n",
    "\n",
    "    # compute for second mouse\n",
    "    \n",
    "    obs_data_2 = obs_data_2.reshape((np.int64(shapes[0]/4),4),order=\"F\")\n",
    "    obs_data_2 = obs_data_2\n",
    "    sim_data_2 = np.reshape(sim_data_2,shape=(np.int64(shapes[0]/4),4),order=\"F\")\n",
    "    sim_data_2 = sim_data_2\n",
    "    #obs_data=obs_data[:,0:4]\n",
    "    #sim_data=sim_data[:,0:4]\n",
    "    # Compute block-wise means and variances for observed and simulated data\n",
    "    obs_means_2, obs_variances_2 = mean_variance_per_block(obs_data_2, block_size, num_blocks)\n",
    "    sim_means_2, sim_variances_2 = mean_variance_per_block(sim_data_2, block_size, num_blocks)\n",
    "\n",
    "    # Compute Euclidean distance for means\n",
    "    distance_means_2 = obs_means_2 - sim_means_2\n",
    "    # Compute Euclidean distance for variances\n",
    "    distance_variances_2 = obs_variances_2 - sim_variances_2\n",
    "\n",
    "    distances = np.vstack((distance_means_1,distance_variances_1,distance_means_2,distance_variances_2))\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dcb80e-9411-4473-bf89-e7aa76f9f466",
   "metadata": {},
   "source": [
    "## generate 1000 simulations from posterior parameter distribution for model 1 and return distances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7848170-9b55-45a2-8f24-07ab2a8ae587",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dictPred_0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m  \n\u001b[1;32m      2\u001b[0m num_clones\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m\n\u001b[0;32m----> 3\u001b[0m sampled_indices \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mdictPred_0\u001b[49m)), n)\n\u001b[1;32m      4\u001b[0m Pred_0 \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m distPred_0 \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dictPred_0' is not defined"
     ]
    }
   ],
   "source": [
    "n = 1000  \n",
    "num_clones=200\n",
    "sampled_indices = random.sample(range(len(dictPred_0)), n)\n",
    "Pred_0 = []\n",
    "distPred_0 = []\n",
    "dist2Pred_0 = []\n",
    "dist3Pred_0 = []\n",
    "for i in sampled_indices:\n",
    "    sim_data=bigModel_1(dictPred_0[i])\n",
    "    sim_data_1 = sim_data[\"data\"][0:num_clones*4]\n",
    "    sim_data_2 = sim_data[\"data\"][num_clones*4:]\n",
    "    shapes=np.shape(sim_data_1)\n",
    "    sim_data_1 = np.reshape(sim_data_1,shape=(np.int64(shapes[0]/4),4),order=\"F\")\n",
    "    sim_data_2 = np.reshape(sim_data_2,shape=(np.int64(shapes[0]/4),4),order=\"F\")\n",
    "    predReorg=np.vstack((sim_data_1,sim_data_2))\n",
    "    Pred_0.append(predReorg) \n",
    "    distPred_0.append(distance(bigModel_1(dictPred_0[i]), {\"data\": observation}))\n",
    "    dist2Pred_0.append(distance_2(bigModel_1(dictPred_0[i]),{\"data\":observation}))\n",
    "    dist3Pred_0.append(distance_3(bigModel_1(dictPred_0[i]),{\"data\":observation}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac9cc42-826e-48bf-a200-16c8fda6821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we generate 1000 simulations from the posterior distribution, here for model 3\n",
    "Pred_3 = []\n",
    "distPred_3 = []\n",
    "dist2Pred_3 = []\n",
    "dist3Pred_3 = []\n",
    "n = 1000  \n",
    "num_clones=200\n",
    "sampled_indices = np.random.choice(range(len(dictPred_3)), n)\n",
    "Pred_3 = []\n",
    "for i in sampled_indices:\n",
    "    sim_data=bigModel_4(dictPred_3[i])\n",
    "    sim_data_1 = sim_data[\"data\"][0:num_clones*4]\n",
    "    sim_data_2 = sim_data[\"data\"][num_clones*4:]\n",
    "    shapes=np.shape(sim_data_1)\n",
    "    sim_data_1 = np.reshape(sim_data_1,shape=(np.int64(shapes[0]/4),4),order=\"F\")\n",
    "    sim_data_2 = np.reshape(sim_data_2,shape=(np.int64(shapes[0]/4),4),order=\"F\")\n",
    "    predReorg=np.vstack((sim_data_1,sim_data_2))\n",
    "    Pred_3.append(predReorg) \n",
    "    distPred_3.append(distance(bigModel_4(dictPred_3[i]), {\"data\": observation}))\n",
    "    dist2Pred_3.append(distance_2(bigModel_4(dictPred_3[i]),{\"data\":observation}))\n",
    "    dist3Pred_3.append(distance_3(bigModel_4(dictPred_3[i]),{\"data\":observation}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95da34c5-1e14-4268-b364-d5b106da9c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pred_0=np.reshape(Pred_0,shape=(400000,4))\n",
    "Pred_3=np.reshape(Pred_3,shape=(400000,4))\n",
    "np.savetxt(\"predCRModel0.csv\", Pred_0, delimiter=\",\")\n",
    "\n",
    "np.savetxt(\"predCRModel3.csv\", Pred_3, delimiter=\",\")\n",
    "\n",
    "\n",
    "\n",
    "dist3Pred_0=np.reshape(dist3Pred_0,shape=(28000,4))\n",
    "dist3Pred_3=np.reshape(dist3Pred_3,shape=(28000,4))\n",
    "np.savetxt(\"dist3CRModel0.csv\", dist3Pred_0, delimiter=\",\")\n",
    "np.savetxt(\"dist3CRModel3.csv\", dist3Pred_3, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyabc",
   "language": "python",
   "name": "pyabc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
